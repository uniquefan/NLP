{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "f = open('text.txt',mode='r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "   \n",
    "    # lower case text\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    # remove punctuations\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    long_words=[]\n",
    "    # remove short word\n",
    "    for i in newString.split():\n",
    "        if len(i)>=3:                  \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "# preprocess the text\n",
    "text = f.read()\n",
    "data_new = text_cleaner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 7052\n"
     ]
    }
   ],
   "source": [
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        # select sequence of tokens\n",
    "        seq = text[i-length:i+1]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# create sequences   \n",
    "sequences = create_seq(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a character mapping index\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        # integer encode line\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6346, 30) Val shape: (706, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "# create X and y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 50)            1350      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 150)               90900     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 27)                4077      \n",
      "=================================================================\n",
      "Total params: 96,327\n",
      "Trainable params: 96,327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "199/199 - 8s - loss: 2.7641 - acc: 0.2042 - val_loss: 2.4358 - val_acc: 0.2748\n",
      "Epoch 2/100\n",
      "199/199 - 6s - loss: 2.3085 - acc: 0.3158 - val_loss: 2.2548 - val_acc: 0.3173\n",
      "Epoch 3/100\n",
      "199/199 - 7s - loss: 2.1736 - acc: 0.3531 - val_loss: 2.1594 - val_acc: 0.3484\n",
      "Epoch 4/100\n",
      "199/199 - 6s - loss: 2.0701 - acc: 0.3815 - val_loss: 2.0899 - val_acc: 0.3839\n",
      "Epoch 5/100\n",
      "199/199 - 6s - loss: 1.9773 - acc: 0.4032 - val_loss: 2.0379 - val_acc: 0.3938\n",
      "Epoch 6/100\n",
      "199/199 - 7s - loss: 1.8920 - acc: 0.4368 - val_loss: 2.0016 - val_acc: 0.4150\n",
      "Epoch 7/100\n",
      "199/199 - 6s - loss: 1.8218 - acc: 0.4497 - val_loss: 1.9559 - val_acc: 0.4178\n",
      "Epoch 8/100\n",
      "199/199 - 7s - loss: 1.7413 - acc: 0.4669 - val_loss: 1.9160 - val_acc: 0.4589\n",
      "Epoch 9/100\n",
      "199/199 - 7s - loss: 1.6684 - acc: 0.4992 - val_loss: 1.8936 - val_acc: 0.4603\n",
      "Epoch 10/100\n",
      "199/199 - 6s - loss: 1.5942 - acc: 0.5202 - val_loss: 1.8616 - val_acc: 0.4717\n",
      "Epoch 11/100\n",
      "199/199 - 6s - loss: 1.5361 - acc: 0.5351 - val_loss: 1.8463 - val_acc: 0.4788\n",
      "Epoch 12/100\n",
      "199/199 - 6s - loss: 1.4620 - acc: 0.5528 - val_loss: 1.8413 - val_acc: 0.4929\n",
      "Epoch 13/100\n",
      "199/199 - 6s - loss: 1.3977 - acc: 0.5698 - val_loss: 1.8544 - val_acc: 0.4943\n",
      "Epoch 14/100\n",
      "199/199 - 6s - loss: 1.3362 - acc: 0.5900 - val_loss: 1.8546 - val_acc: 0.4943\n",
      "Epoch 15/100\n",
      "199/199 - 6s - loss: 1.2830 - acc: 0.6083 - val_loss: 1.8533 - val_acc: 0.4901\n",
      "Epoch 16/100\n",
      "199/199 - 6s - loss: 1.2202 - acc: 0.6272 - val_loss: 1.8610 - val_acc: 0.4958\n",
      "Epoch 17/100\n",
      "199/199 - 6s - loss: 1.1775 - acc: 0.6352 - val_loss: 1.8817 - val_acc: 0.4816\n",
      "Epoch 18/100\n",
      "199/199 - 5s - loss: 1.1230 - acc: 0.6500 - val_loss: 1.8797 - val_acc: 0.5057\n",
      "Epoch 19/100\n",
      "199/199 - 6s - loss: 1.0792 - acc: 0.6620 - val_loss: 1.9022 - val_acc: 0.4788\n",
      "Epoch 20/100\n",
      "199/199 - 6s - loss: 1.0313 - acc: 0.6823 - val_loss: 1.9129 - val_acc: 0.4972\n",
      "Epoch 21/100\n",
      "199/199 - 6s - loss: 0.9962 - acc: 0.6842 - val_loss: 1.9241 - val_acc: 0.5042\n",
      "Epoch 22/100\n",
      "199/199 - 6s - loss: 0.9480 - acc: 0.6984 - val_loss: 1.9604 - val_acc: 0.4830\n",
      "Epoch 23/100\n",
      "199/199 - 6s - loss: 0.9206 - acc: 0.7082 - val_loss: 1.9819 - val_acc: 0.5028\n",
      "Epoch 24/100\n",
      "199/199 - 6s - loss: 0.8891 - acc: 0.7238 - val_loss: 2.0070 - val_acc: 0.4972\n",
      "Epoch 25/100\n",
      "199/199 - 6s - loss: 0.8435 - acc: 0.7411 - val_loss: 2.0337 - val_acc: 0.4943\n",
      "Epoch 26/100\n",
      "199/199 - 5s - loss: 0.8165 - acc: 0.7430 - val_loss: 2.0618 - val_acc: 0.4929\n",
      "Epoch 27/100\n",
      "199/199 - 6s - loss: 0.7896 - acc: 0.7531 - val_loss: 2.0912 - val_acc: 0.4887\n",
      "Epoch 28/100\n",
      "199/199 - 6s - loss: 0.7672 - acc: 0.7551 - val_loss: 2.1138 - val_acc: 0.5057\n",
      "Epoch 29/100\n",
      "199/199 - 6s - loss: 0.7364 - acc: 0.7699 - val_loss: 2.1365 - val_acc: 0.4802\n",
      "Epoch 30/100\n",
      "199/199 - 6s - loss: 0.7112 - acc: 0.7775 - val_loss: 2.1682 - val_acc: 0.5042\n",
      "Epoch 31/100\n",
      "199/199 - 6s - loss: 0.6907 - acc: 0.7873 - val_loss: 2.1900 - val_acc: 0.4887\n",
      "Epoch 32/100\n",
      "199/199 - 5s - loss: 0.6696 - acc: 0.7917 - val_loss: 2.1971 - val_acc: 0.4958\n",
      "Epoch 33/100\n",
      "199/199 - 6s - loss: 0.6499 - acc: 0.7953 - val_loss: 2.2248 - val_acc: 0.4943\n",
      "Epoch 34/100\n",
      "199/199 - 6s - loss: 0.6334 - acc: 0.7983 - val_loss: 2.2481 - val_acc: 0.5000\n",
      "Epoch 35/100\n",
      "199/199 - 6s - loss: 0.6221 - acc: 0.8101 - val_loss: 2.3035 - val_acc: 0.5042\n",
      "Epoch 36/100\n",
      "199/199 - 6s - loss: 0.5998 - acc: 0.8085 - val_loss: 2.3289 - val_acc: 0.4901\n",
      "Epoch 37/100\n",
      "199/199 - 6s - loss: 0.5804 - acc: 0.8148 - val_loss: 2.3772 - val_acc: 0.4816\n",
      "Epoch 38/100\n",
      "199/199 - 6s - loss: 0.5635 - acc: 0.8240 - val_loss: 2.4104 - val_acc: 0.4915\n",
      "Epoch 39/100\n",
      "199/199 - 6s - loss: 0.5538 - acc: 0.8234 - val_loss: 2.4032 - val_acc: 0.4943\n",
      "Epoch 40/100\n",
      "199/199 - 6s - loss: 0.5408 - acc: 0.8270 - val_loss: 2.4056 - val_acc: 0.4972\n",
      "Epoch 41/100\n",
      "199/199 - 6s - loss: 0.5289 - acc: 0.8295 - val_loss: 2.4216 - val_acc: 0.5099\n",
      "Epoch 42/100\n",
      "199/199 - 6s - loss: 0.5223 - acc: 0.8309 - val_loss: 2.5188 - val_acc: 0.4873\n",
      "Epoch 43/100\n",
      "199/199 - 5s - loss: 0.5069 - acc: 0.8421 - val_loss: 2.5246 - val_acc: 0.4915\n",
      "Epoch 44/100\n",
      "199/199 - 6s - loss: 0.4968 - acc: 0.8382 - val_loss: 2.5244 - val_acc: 0.4972\n",
      "Epoch 45/100\n",
      "199/199 - 6s - loss: 0.4916 - acc: 0.8435 - val_loss: 2.5236 - val_acc: 0.4972\n",
      "Epoch 46/100\n",
      "199/199 - 6s - loss: 0.4911 - acc: 0.8421 - val_loss: 2.5765 - val_acc: 0.4887\n",
      "Epoch 47/100\n",
      "199/199 - 8s - loss: 0.4690 - acc: 0.8514 - val_loss: 2.5763 - val_acc: 0.4788\n",
      "Epoch 48/100\n",
      "199/199 - 6s - loss: 0.4574 - acc: 0.8555 - val_loss: 2.6069 - val_acc: 0.4915\n",
      "Epoch 49/100\n",
      "199/199 - 6s - loss: 0.4676 - acc: 0.8467 - val_loss: 2.6450 - val_acc: 0.4816\n",
      "Epoch 50/100\n",
      "199/199 - 6s - loss: 0.4480 - acc: 0.8553 - val_loss: 2.6616 - val_acc: 0.4915\n",
      "Epoch 51/100\n",
      "199/199 - 6s - loss: 0.4372 - acc: 0.8626 - val_loss: 2.7210 - val_acc: 0.4887\n",
      "Epoch 52/100\n",
      "199/199 - 6s - loss: 0.4311 - acc: 0.8623 - val_loss: 2.7295 - val_acc: 0.4929\n",
      "Epoch 53/100\n",
      "199/199 - 6s - loss: 0.4243 - acc: 0.8664 - val_loss: 2.7023 - val_acc: 0.4929\n",
      "Epoch 54/100\n",
      "199/199 - 6s - loss: 0.4249 - acc: 0.8607 - val_loss: 2.7887 - val_acc: 0.4915\n",
      "Epoch 55/100\n",
      "199/199 - 6s - loss: 0.4110 - acc: 0.8645 - val_loss: 2.8198 - val_acc: 0.4986\n",
      "Epoch 56/100\n",
      "199/199 - 6s - loss: 0.4100 - acc: 0.8623 - val_loss: 2.8057 - val_acc: 0.5028\n",
      "Epoch 57/100\n",
      "199/199 - 6s - loss: 0.3973 - acc: 0.8735 - val_loss: 2.8327 - val_acc: 0.5014\n",
      "Epoch 58/100\n",
      "199/199 - 6s - loss: 0.3957 - acc: 0.8746 - val_loss: 2.8348 - val_acc: 0.4958\n",
      "Epoch 59/100\n",
      "199/199 - 6s - loss: 0.3888 - acc: 0.8692 - val_loss: 2.8881 - val_acc: 0.4958\n",
      "Epoch 60/100\n",
      "199/199 - 6s - loss: 0.3796 - acc: 0.8810 - val_loss: 2.9085 - val_acc: 0.4929\n",
      "Epoch 61/100\n",
      "199/199 - 6s - loss: 0.3837 - acc: 0.8750 - val_loss: 2.8983 - val_acc: 0.4972\n",
      "Epoch 62/100\n",
      "199/199 - 6s - loss: 0.3840 - acc: 0.8747 - val_loss: 2.9504 - val_acc: 0.4830\n",
      "Epoch 63/100\n",
      "199/199 - 6s - loss: 0.3817 - acc: 0.8749 - val_loss: 2.9834 - val_acc: 0.4759\n",
      "Epoch 64/100\n",
      "199/199 - 6s - loss: 0.3632 - acc: 0.8823 - val_loss: 3.0092 - val_acc: 0.4745\n",
      "Epoch 65/100\n",
      "199/199 - 6s - loss: 0.3575 - acc: 0.8886 - val_loss: 2.9910 - val_acc: 0.4802\n",
      "Epoch 66/100\n",
      "199/199 - 6s - loss: 0.3520 - acc: 0.8872 - val_loss: 3.0148 - val_acc: 0.4844\n",
      "Epoch 67/100\n",
      "199/199 - 6s - loss: 0.3605 - acc: 0.8765 - val_loss: 3.0328 - val_acc: 0.4844\n",
      "Epoch 68/100\n",
      "199/199 - 6s - loss: 0.3512 - acc: 0.8821 - val_loss: 3.0613 - val_acc: 0.4844\n",
      "Epoch 69/100\n",
      "199/199 - 6s - loss: 0.3566 - acc: 0.8823 - val_loss: 3.0861 - val_acc: 0.4858\n",
      "Epoch 70/100\n",
      "199/199 - 5s - loss: 0.3360 - acc: 0.8913 - val_loss: 3.0590 - val_acc: 0.4745\n",
      "Epoch 71/100\n",
      "199/199 - 6s - loss: 0.3451 - acc: 0.8859 - val_loss: 3.1214 - val_acc: 0.4660\n",
      "Epoch 72/100\n",
      "199/199 - 6s - loss: 0.3418 - acc: 0.8894 - val_loss: 3.1127 - val_acc: 0.4688\n",
      "Epoch 73/100\n",
      "199/199 - 5s - loss: 0.3348 - acc: 0.8925 - val_loss: 3.1150 - val_acc: 0.4745\n",
      "Epoch 74/100\n",
      "199/199 - 8s - loss: 0.3316 - acc: 0.8936 - val_loss: 3.1821 - val_acc: 0.4802\n",
      "Epoch 75/100\n",
      "199/199 - 6s - loss: 0.3306 - acc: 0.8925 - val_loss: 3.2159 - val_acc: 0.4603\n",
      "Epoch 76/100\n",
      "199/199 - 6s - loss: 0.3339 - acc: 0.8899 - val_loss: 3.2066 - val_acc: 0.4731\n",
      "Epoch 77/100\n",
      "199/199 - 6s - loss: 0.3175 - acc: 0.8928 - val_loss: 3.2297 - val_acc: 0.4646\n",
      "Epoch 78/100\n",
      "199/199 - 6s - loss: 0.3224 - acc: 0.8921 - val_loss: 3.1825 - val_acc: 0.4759\n",
      "Epoch 79/100\n",
      "199/199 - 6s - loss: 0.3332 - acc: 0.8870 - val_loss: 3.2020 - val_acc: 0.4688\n",
      "Epoch 80/100\n",
      "199/199 - 6s - loss: 0.3182 - acc: 0.8930 - val_loss: 3.2414 - val_acc: 0.4618\n",
      "Epoch 81/100\n",
      "199/199 - 6s - loss: 0.3114 - acc: 0.8993 - val_loss: 3.1906 - val_acc: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "199/199 - 6s - loss: 0.3009 - acc: 0.9042 - val_loss: 3.2285 - val_acc: 0.4745\n",
      "Epoch 83/100\n",
      "199/199 - 6s - loss: 0.3067 - acc: 0.9015 - val_loss: 3.2522 - val_acc: 0.4589\n",
      "Epoch 84/100\n",
      "199/199 - 6s - loss: 0.2994 - acc: 0.8973 - val_loss: 3.2699 - val_acc: 0.4688\n",
      "Epoch 85/100\n",
      "199/199 - 6s - loss: 0.3075 - acc: 0.8980 - val_loss: 3.2888 - val_acc: 0.4660\n",
      "Epoch 86/100\n",
      "199/199 - 6s - loss: 0.2964 - acc: 0.9026 - val_loss: 3.2816 - val_acc: 0.4759\n",
      "Epoch 87/100\n",
      "199/199 - 5s - loss: 0.2991 - acc: 0.9006 - val_loss: 3.3169 - val_acc: 0.4632\n",
      "Epoch 88/100\n",
      "199/199 - 6s - loss: 0.2949 - acc: 0.9017 - val_loss: 3.3677 - val_acc: 0.4618\n",
      "Epoch 89/100\n",
      "199/199 - 6s - loss: 0.2965 - acc: 0.8969 - val_loss: 3.3484 - val_acc: 0.4688\n",
      "Epoch 90/100\n",
      "199/199 - 6s - loss: 0.2850 - acc: 0.9075 - val_loss: 3.4219 - val_acc: 0.4745\n",
      "Epoch 91/100\n",
      "199/199 - 6s - loss: 0.2817 - acc: 0.9099 - val_loss: 3.3936 - val_acc: 0.4802\n",
      "Epoch 92/100\n",
      "199/199 - 6s - loss: 0.2785 - acc: 0.9086 - val_loss: 3.4504 - val_acc: 0.4717\n",
      "Epoch 93/100\n",
      "199/199 - 6s - loss: 0.2922 - acc: 0.9018 - val_loss: 3.4289 - val_acc: 0.4745\n",
      "Epoch 94/100\n",
      "199/199 - 6s - loss: 0.2921 - acc: 0.9015 - val_loss: 3.4379 - val_acc: 0.4688\n",
      "Epoch 95/100\n",
      "199/199 - 6s - loss: 0.2824 - acc: 0.9026 - val_loss: 3.4306 - val_acc: 0.4688\n",
      "Epoch 96/100\n",
      "199/199 - 6s - loss: 0.2796 - acc: 0.9099 - val_loss: 3.4800 - val_acc: 0.4703\n",
      "Epoch 97/100\n",
      "199/199 - 6s - loss: 0.2858 - acc: 0.9073 - val_loss: 3.4705 - val_acc: 0.4830\n",
      "Epoch 98/100\n",
      "199/199 - 6s - loss: 0.2657 - acc: 0.9129 - val_loss: 3.4722 - val_acc: 0.4802\n",
      "Epoch 99/100\n",
      "199/199 - 6s - loss: 0.2649 - acc: 0.9152 - val_loss: 3.4892 - val_acc: 0.4660\n",
      "Epoch 100/100\n",
      "199/199 - 6s - loss: 0.2707 - acc: 0.9130 - val_loss: 3.5318 - val_acc: 0.4717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c2e4875848>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "# fit the model\n",
    "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Itak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what the most barbalowe for the support this declaration with firm refused his assent laws the most wholesom'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_seq(model,mapping,30,'what the',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cha_lm\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('cha_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('cha_lm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
